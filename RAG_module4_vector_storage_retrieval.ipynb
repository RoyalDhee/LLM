{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0d9e3b",
   "metadata": {},
   "source": [
    "### Faiss Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25160c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS version: 1.13.1\n",
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(f\"FAISS version: {faiss.__version__}\")\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd68068",
   "metadata": {},
   "source": [
    "### Prepare Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540234f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 10\n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data science.\",\n",
    "    \"Machine learning models require large amounts of training data to perform well.\",\n",
    "    \"Neural networks are inspired by the structure of the human brain.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning is a subset of machine learning using multi-layered neural networks.\",\n",
    "    \"Data visualization helps communicate insights from complex datasets.\",\n",
    "    \"Cloud computing provides on-demand access to computing resources.\",\n",
    "    \"Cybersecurity protects systems and networks from digital attacks.\",\n",
    "    \"Blockchain technology enables secure, decentralized transactions.\",\n",
    "    \"Quantum computing uses quantum mechanics to solve complex problems.\"\n",
    "]\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b02b84",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a1a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 embeddings\n",
      "Each embedding has 384 dimensions\n",
      "Embeddings shape: (10, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Each embedding has {embeddings.shape[1]} dimensions\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768adda",
   "metadata": {},
   "source": [
    "### Create FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8b983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index created!\n",
      "Total vectors in index: 10\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings dimension\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index (IndexFlatL2 = exact search with L2 distance)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"‚úÖ FAISS index created!\")\n",
    "print(f\"Total vectors in index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f4f70b",
   "metadata": {},
   "source": [
    "### Search with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf476155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: WHat is artificial intelligence and machine learning\n",
      "\n",
      "Top 3 results:\n",
      "\n",
      "1. (distance: 0.9125)\n",
      "  Deep learning is a subset of machine learning using multi-layered neural networks.\n",
      "\n",
      "2. (distance: 1.1951)\n",
      "  Machine learning models require large amounts of training data to perform well.\n",
      "\n",
      "3. (distance: 1.2401)\n",
      "  Natural language processing enables computers to understand human language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query = \"WHat is artificial intelligence and machine learning\"\n",
    "\n",
    "# Embed query\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Search: find top 3 most similar vectors\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Top {k} results:\\n\")\n",
    "\n",
    "for i, (idx, distance) in enumerate(zip(indices[0], distances[0]), 1):\n",
    "  print(f\"{i}. (distance: {distance:.4f})\")\n",
    "  print(f\"  {documents[idx]}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d810a92",
   "metadata": {},
   "source": [
    "### Using Cosine SImilarity with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "500748d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: WHat is artificial intelligence and machine learning\n",
      "\n",
      "Top 3 results with cosine similarity:\n",
      "\n",
      "1. (Similarity: 0.5437)\n",
      "   Deep learning is a subset of machine learning using multi-layered neural networks.\n",
      "\n",
      "2. (Similarity: 0.4025)\n",
      "   Machine learning models require large amounts of training data to perform well.\n",
      "\n",
      "3. (Similarity: 0.3799)\n",
      "   Natural language processing enables computers to understand human language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings for cosine similarity\n",
    "embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Create index with inner product (equivalent to cosine for normalized vectors)\n",
    "index_cosine = faiss.IndexFlatIP(dimension)\n",
    "index_cosine.add(embeddings_normalized)\n",
    "\n",
    "# Search with normalized query\n",
    "query_embedding_normalized = query_embedding / np.linalg.norm(query_embedding)\n",
    "scores, indices = index_cosine.search(query_embedding_normalized, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Top {k} results with cosine similarity:\\n\")\n",
    "\n",
    "for i, (idx, score) in enumerate(zip(indices[0], scores[0]), 1):\n",
    "    print(f\"{i}. (Similarity: {score:.4f})\")\n",
    "    print(f\"   {documents[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d70fc",
   "metadata": {},
   "source": [
    "### Saving and Loading FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66008a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index saved to disk\n",
      "Documents saved\n"
     ]
    }
   ],
   "source": [
    "# Save index to disk\n",
    "faiss.write_index(index_cosine, \"my_faiss_index.bin\")\n",
    "print(\"‚úÖ Index saved to disk\")\n",
    "\n",
    "# Save documents seperately (FAISS only store vectors, not text)\n",
    "import pickle\n",
    "with open(\"documents.pkl\", 'wb') as f:\n",
    "  pickle.dump(documents, f)\n",
    "print(\"Documents saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a0f9029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded: 10 vectors\n",
      "Documents loaded: 10 documents\n"
     ]
    }
   ],
   "source": [
    "# Load index from disk\n",
    "loaded_index = faiss.read_index(\"my_faiss_index.bin\")\n",
    "print(f\"Index loaded: {loaded_index.ntotal} vectors\")\n",
    "\n",
    "# Load documents\n",
    "with open(\"documents.pkl\", \"rb\") as f:\n",
    "  loaded_documents = pickle.load(f)\n",
    "print(f\"Documents loaded: {len(loaded_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a155a313",
   "metadata": {},
   "source": [
    "### Chroma Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ffd1749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB version: 1.3.6\n",
      "‚úÖ ChromaDB imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "print(f\"ChromaDB version: {chromadb.__version__}\")\n",
    "print(\"‚úÖ ChromaDB imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de664f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Collection created: my_documents\n",
      "Current count: 0 documents\n",
      "üìÅ Data persisted to: ./chroma_db/\n"
     ]
    }
   ],
   "source": [
    "# Create Chroma client (persistent storage)\n",
    "# Note: ChromaDB 0.4.0+ uses PersistentClient instead of Client(Settings(...))\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create or get collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"my_documents\",\n",
    "    metadata={\"description\": \"Sample document collection\"}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Collection created: {collection.name}\")\n",
    "print(f\"Current count: {collection.count()} documents\")\n",
    "print(f\"üìÅ Data persisted to: ./chroma_db/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95450de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents with metadata\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data science.\",\n",
    "    \"Machine learning models require large amounts of training data to perform well.\",\n",
    "    \"Neural networks are inspired by the structure of the human brain.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning is a subset of machine learning using multi-layered neural networks.\"\n",
    "]\n",
    "\n",
    "# Metadata for each document\n",
    "metadatas = [\n",
    "  {\"category\": \"programming\", \"topic\": \"python\"},\n",
    "  {\"category\": \"AI\", \"topic\": \"machine learning\"},\n",
    "  {\"category\": \"AI\", \"topic\": \"neural networks\"},\n",
    "  {\"category\": \"AI\", \"topic\": \"NLP\"},\n",
    "  {\"category\": \"AI\", \"topic\": \"deep learning\"},\n",
    "  \n",
    "]\n",
    "\n",
    "# IDs for each document\n",
    "ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "\n",
    "# Add to collection (Chroma handles embeding automatically!)\n",
    "collection.add(\n",
    "  documents=documents,\n",
    "  metadatas=metadatas,\n",
    "  ids=ids\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Added {len(documents)} documents to collection\")\n",
    "print(f\"Total documents: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e88403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection.query(\n",
    "  query_texts=[\"What is artificial intelligence\"],\n",
    "  n_results=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6434a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection.query(\n",
    "    query_texts=[\"What is artificial intelligence?\"],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(\"Query: What is artificial intelligence?\\n\")\n",
    "print(\"Top 3 results:\\n\")\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c176994",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7e73a",
   "metadata": {},
   "source": [
    "### Filtering with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUery the metadata filter\n",
    "results = collection.query(\n",
    "  query_texts=[\"Tell me about AI\"],\n",
    "  n_results=3,\n",
    "  where={\"category\":\"AI\"} \n",
    ")\n",
    "\n",
    "print(\"Query: Tell me about AI (filtered by category='AI')\\n\")\n",
    "print(\"Results:\\n\")\n",
    "\n",
    "for i, (doc, metadata) in enumerate(zip(\n",
    "  results['documents'][0],\n",
    "  results['metadatas'][0]\n",
    "), 1):\n",
    "  print(f\"{i}. {doc}\")\n",
    "  print(f\"    Category: {metadata['category']}, Topic: {metadata['topic']}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e1fed",
   "metadata": {},
   "source": [
    "### Using Custom Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Use sentence-transformers embedding function\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "  model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create new collection with custom embedding function\n",
    "collection_custom = client.get_or_create_collection(\n",
    "  name=\"custom_embeddings\",\n",
    "  embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "collection_custoom.add(\n",
    "  documents=documents,\n",
    "  metadatas=metadatas,\n",
    "  ids=ids\n",
    ")\n",
    "\n",
    "print(f\"‚úîÔ∏èCollection with custom embeddings created\")\n",
    "print(f\"Documents: {collection_custom.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection_custom.query(\n",
    "    query_texts=[\"What is artificial intelligence?\"],\n",
    "    n_results=3,\n",
    "    include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb36804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection_custom.query(\n",
    "    query_texts=[\"What is artificial intelligence?\"],\n",
    "    n_results=3,\n",
    "    include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"Query: What is artificial intelligence?\\n\")\n",
    "print(\"Top 3 results:\\n\")\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4e5d4",
   "metadata": {},
   "source": [
    "### Update and Delete Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88697eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update a document\n",
    "collection.update(\n",
    "    ids=[\"doc_0\"],\n",
    "    documents=[\"Python is an amazing programming language for AI and data science!\"],\n",
    "    metadatas=[{\"category\": \"programming\", \"topic\": \"python\", \"updated\": True}]\n",
    ")\n",
    "print(\"‚úÖ Document updated\")\n",
    "\n",
    "# Delete a document\n",
    "# collection.delete(ids=[\"doc_4\"])\n",
    "# print(\"‚úÖ Document deleted\")\n",
    "\n",
    "print(f\"\\nTotal documents after update: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0174a",
   "metadata": {},
   "source": [
    "### Building a complete RAG Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495100f",
   "metadata": {},
   "source": [
    "### RAG Retriever with Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0905f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class RAGRetriever:\n",
    "    def __init__(self, collection_name=\"rag_collection\", persist_dir=\"./rag_db\"):\n",
    "        \"\"\"\n",
    "        Initialize RAG retriever with Chroma.\n",
    "        \"\"\"\n",
    "        # Create Chroma client (using PersistentClient for ChromaDB 0.4.0+)\n",
    "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
    "        \n",
    "        # Create collection with sentence-transformers\n",
    "        embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        \n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=embedding_fn\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ RAG Retriever initialized\")\n",
    "        print(f\"Collection: {collection_name}\")\n",
    "        print(f\"Current documents: {self.collection.count()}\")\n",
    "        print(f\"üìÅ Data persisted to: {persist_dir}/\")\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size=500):\n",
    "        \"\"\"\n",
    "        Simple sentence-based chunking from Module 2.\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def add_document(self, text, metadata=None, source_name=\"unknown\"):\n",
    "        \"\"\"\n",
    "        Add a document (chunks it automatically).\n",
    "        \"\"\"\n",
    "        # Chunk the document\n",
    "        chunks = self.chunk_text(text)\n",
    "        \n",
    "        # Prepare data for Chroma\n",
    "        ids = [f\"{source_name}_chunk_{i}\" for i in range(len(chunks))]\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"source\": source_name,\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                **(metadata or {})\n",
    "            }\n",
    "            for i in range(len(chunks))\n",
    "        ]\n",
    "        \n",
    "        # Add to collection\n",
    "        self.collection.add(\n",
    "            documents=chunks,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Added document '{source_name}': {len(chunks)} chunks\")\n",
    "        return len(chunks)\n",
    "    \n",
    "    def retrieve(self, query, top_k=3, filter_metadata=None):\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks for a query.\n",
    "        \"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=top_k,\n",
    "            where=filter_metadata\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'documents': results['documents'][0],\n",
    "            'metadatas': results['metadatas'][0],\n",
    "            'distances': results['distances'][0]\n",
    "        }\n",
    "    \n",
    "    def format_context(self, retrieved_results):\n",
    "        \"\"\"\n",
    "        Format retrieved chunks for LLM prompt.\n",
    "        \"\"\"\n",
    "        context = \"Context from retrieved documents:\\n\\n\"\n",
    "        \n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            retrieved_results['documents'],\n",
    "            retrieved_results['metadatas'],\n",
    "            retrieved_results['distances']\n",
    "        ), 1):\n",
    "            source = metadata.get('source', 'unknown')\n",
    "            context += f\"[{i}] From {source} (Relevance: {1/(1+distance):.3f}):\\n\"\n",
    "            context += f\"{doc}\\n\\n\"\n",
    "        \n",
    "        return context\n",
    "\n",
    "print(\"‚úÖ RAGRetriever class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4c3cd",
   "metadata": {},
   "source": [
    "### Test the RAG Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd27eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = RAGRetriever(collection_name=\"test_rag\")\n",
    "\n",
    "# Add sample documents\n",
    "doc1 = \"\"\"\n",
    "Machine learning is a branch of artificial intelligence that focuses on building systems \n",
    "that can learn from data. These systems improve their performance over time without being \n",
    "explicitly programmed. Common applications include image recognition, natural language \n",
    "processing, and recommendation systems.\n",
    "\"\"\"\n",
    "\n",
    "doc2 = \"\"\"\n",
    "Python is a high-level programming language known for its simplicity and readability. \n",
    "It's widely used in web development, data science, automation, and artificial intelligence. \n",
    "Python's extensive library ecosystem makes it ideal for rapid development.\n",
    "\"\"\"\n",
    "\n",
    "doc3 = \"\"\"\n",
    "Vector databases are specialized databases designed to store and query high-dimensional \n",
    "vectors efficiently. They're essential for modern AI applications like semantic search, \n",
    "recommendation systems, and retrieval-augmented generation (RAG). Popular examples include \n",
    "FAISS, Pinecone, and Chroma.\n",
    "\"\"\"\n",
    "\n",
    "# Add documents\n",
    "retriever.add_document(doc1, metadata={\"category\": \"AI\"}, source_name=\"ml_intro.txt\")\n",
    "retriever.add_document(doc2, metadata={\"category\": \"programming\"}, source_name=\"python_guide.txt\")\n",
    "retriever.add_document(doc3, metadata={\"category\": \"databases\"}, source_name=\"vector_db_overview.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bec225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "query = \"What are vector databases used for?\"\n",
    "\n",
    "results = retriever.retrieve(query, top_k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(retriever.format_context(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
