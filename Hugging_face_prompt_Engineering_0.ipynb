{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2b5339",
   "metadata": {},
   "source": [
    "### Working with Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144746e6",
   "metadata": {},
   "source": [
    "#### Reading and Writing Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading files:\n",
    "with open(\"example.txt\", \"r\") as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Files\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "  file.write(\"This is a sample output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading files line-by-line\n",
    "with open(\"example.txt\", \"r\") as file:\n",
    "  for line in file:\n",
    "    print(line.strip()) #Removes extra newline characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115885c2",
   "metadata": {},
   "source": [
    "#### Working with CSV Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0324ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the csv library to process structured text.\n",
    "import csv\n",
    "\n",
    "with open(\"Lagos Precipitation Truncated.csv\", \"r\") as file:\n",
    "  reader = csv.reader(file)\n",
    "  for row in reader:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b688ef",
   "metadata": {},
   "source": [
    "#### Working with JSON Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing from json files\n",
    "import json\n",
    "with open(\"example.json\", \"r\") as file:\n",
    "  data = json.load(file)\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2013a9",
   "metadata": {},
   "source": [
    "### Text Preprocessing for Prompt Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401767e",
   "metadata": {},
   "source": [
    "#### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe96c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization Example\n",
    "text = \"AI is transformimg the world\"\n",
    "words = text.split()\n",
    "print(words) # Output: ['AI', 'is', 'transforming', 'the', 'world.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96569a50",
   "metadata": {},
   "source": [
    "#### 2. Text Normalization:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250209f",
   "metadata": {},
   "source": [
    "Converting text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ec69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Normalization\n",
    "text = \"This is Prompt Engineering.\"\n",
    "print(text.lower()) # Output: this is prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation\n",
    "import string\n",
    "\n",
    "text = \"Hello, World!\"\n",
    "clean_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # removes any of !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "print(clean_text)   # Output: Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e5b40",
   "metadata": {},
   "source": [
    "#### 3. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8532b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca5576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using nltk.stem for stemming and lemmatization\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = porterStemmer()\n",
    "words = [\"running\", \"runner\", \"ran\", \"documented\", \"decorating\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)    # Output: ['run', 'runner', 'ran']\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"runner\", \"ran\", \"documented\", \"accelerated\", \"decorating\", \"going\", \"does\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b5c27",
   "metadata": {},
   "source": [
    "#### 4. Stopwords Removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords:\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = [\"this\", \"is\", \"an\", \"example\"]\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "print(filtered_words) # Output: ['example']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63243f6",
   "metadata": {},
   "source": [
    "#### 5. Removing Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Removal:\n",
    "import re   #Using regex\n",
    "\n",
    "text = \"AI123 is transforming the world! @OpenAI\"\n",
    "clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "print(clean_text) #Output: AI is transforming the world OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae45c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Read, clean, and tokenize a text file\n",
    "def read_and_tokenize(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Clean the text (remove punctuation and convert to lowercase)\n",
    "        clean_text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "        words = clean_text.split()\n",
    "        return words\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found. Please provide a valid file path.\"\n",
    "\n",
    "# Step 2: Remove stopwords and punctuation\n",
    "def remove_stopwords_and_punctuation(words):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Step 3: Generate a simple prompt\n",
    "def generate_prompt(cleaned_text):\n",
    "    prompt = f\"Based on the following input, provide insights: {cleaned_text}\"\n",
    "    return prompt\n",
    "\n",
    "# Example workflow\n",
    "file_path = \"example.txt\"  # Replace with your file path\n",
    "tokens = read_and_tokenize(file_path)\n",
    "cleaned_text = remove_stopwords_and_punctuation(tokens)\n",
    "prompt = generate_prompt(cleaned_text)\n",
    "\n",
    "print(\"Generated Prompt:\", prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
