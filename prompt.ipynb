{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52aed994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the key concepts of the text delimited by triple backticks in simple terms. ```Machine learning is a field of artificial intelligence that enables computers to learn from data and make predictions```\n"
     ]
    }
   ],
   "source": [
    "text = \"Machine learning is a field of artificial intelligence that enables computers to learn from data and make predictions\"\n",
    "\n",
    "prompt = f\"\"\"Explain the key concepts of the text delimited by triple backticks in simple terms. ```{text}```\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9540aa",
   "metadata": {},
   "source": [
    "Learn restful architecture, flask and jinja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31e152",
   "metadata": {},
   "source": [
    "**Zero Shot Prompt** \n",
    "\n",
    "Not providing the model with an example before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01957999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems there might be a misunderstanding about the original language. The sentence \"Oju ojo yii o da leni\" is **not Spanish**; it is **Yoruba**.\n",
      "\n",
      "In Yoruba, \"Oju ojo yii o da leni\" means \"The weather is good today.\"\n",
      "\n",
      "Here's the translation into French:\n",
      "\n",
      "**Il fait beau aujourd'hui.**\n",
      "\n",
      "You could also say:\n",
      "*   **Le temps est bon aujourd'hui.**\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate the following Spanish sentence into French: 'Oju ojo yii o da leni'\"\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c6fa56",
   "metadata": {},
   "source": [
    "**One Shot Prompt** \n",
    "\n",
    "Providing the model with a **single or one** example before asking it to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Detected the language of theeeeee following sentences:\n",
    "text: \"Good morning\" -> language: \"English\"\n",
    "\"text\": A plus tard\" -> language: \"French\"\n",
    "Now detect:\n",
    "text: \"Adupe!\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2933152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"Adupe!\" -> language: \"Yoruba\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24802fac",
   "metadata": {},
   "source": [
    "**Few-shot prompting**\n",
    "\n",
    "few shot prompting gives the model multiple examples to learn the task before requestion response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c9b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Determine the sentiment of the following sentences:\n",
    "text: \"I love this product! It works perfectly.\" -> classification: positive\n",
    "text: \"The service was terrible. I'm very disappointed.\" -> classification: Negative\n",
    "text: \"The food was okay, nothing special.\" -> Classification: Neutral \n",
    "Now analyze this sentence:\n",
    "\"The movie was amazing, I enjoyed every moment of it\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb72297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification: positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a282784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Determine the sentiment of the following sentences:\n",
    "text: \"I love this product! It works perfectly.\" -> classification: positive\n",
    "text: \"The service was terrible. I'm very disappointed.\" -> classification: Negative\n",
    "text: \"The food was okay, nothing special.\" -> Classification: Neutral \n",
    "Now analyze this sentence:\n",
    "\"The country is getting worst day by day, we are blessed with lot of mineral resources\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb4e700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"The country is getting worst day by day, we are blessed with lot of mineral resources\" -> classification: Negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d19d0d",
   "metadata": {},
   "source": [
    "**Multi-step prompting**\n",
    "\n",
    "Multi-step prompting involves breaking down a complex query into smaller, manageable steps to improve model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2afdeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Marie Curie discovered raydium 1898\"\n",
    "\n",
    "prompt = f\"\"\"You will be provided with a text delimited by a triple backticks.\n",
    "Step 1: Identify the key entities in the sentence.\n",
    "Step 2: Classify each entity as a person, event, or object.\n",
    "```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7dca6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Identify the key entities in the sentence.\n",
      "*   Marie Curie\n",
      "*   raydium\n",
      "*   1898\n",
      "\n",
      "Step 2: Classify each entity as a person, event, or object.\n",
      "*   **Marie Curie**: Person\n",
      "*   **raydium**: Object\n",
      "*   **1898**: Event (Represents the year the discovery event occurred)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0f341",
   "metadata": {},
   "source": [
    "**Chain-of-thought Prompting**\n",
    "\n",
    "Chain of thought prompting encourages the model to break down reasoning into logical steps before answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81bc7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"John has 3 apples. He buys 5 more and gives away 2. How many apples does he have now? Think step by step.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337fa9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step:\n",
      "\n",
      "1.  **Starting apples:** John has 3 apples.\n",
      "2.  **Buys more:** He buys 5 more, so $3 + 5 = 8$ apples.\n",
      "3.  **Gives away:** He gives away 2, so $8 - 2 = 6$ apples.\n",
      "\n",
      "John has **6** apples now.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c3c07",
   "metadata": {},
   "source": [
    "**Self Consistency Prompting**\n",
    "\n",
    "self consistency prompting generates multiple responses and selects the most consistent answer to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1beb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Imagine three completely independent experts who reason differently are answering this question. The final answer is obtained by majority vote. The question is: \"\n",
    "question = \"A bookstore has 20 books on a shelf. a customer buys 5 books. Then, the store restocks with double the number of books bought. After that, another customer buys 8 books. How many books are left on the shelf?\"\n",
    "\n",
    "prompt = instruction + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ef5774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step-by-step as three independent experts would, and see if they converge!\n",
      "\n",
      "**Expert 1: The Chronological Tracker**\n",
      "\n",
      "*   **Start:** The shelf has 20 books.\n",
      "*   **Customer 1 buys:** 20 - 5 = 15 books.\n",
      "*   **Store restocks:** The customer bought 5 books. Double that is 5 * 2 = 10 books.\n",
      "    *   So, 15 + 10 = 25 books.\n",
      "*   **Customer 2 buys:** 25 - 8 = 17 books.\n",
      "\n",
      "**Expert 2: The Net Change Analyst**\n",
      "\n",
      "*   **Initial state:** +20 books.\n",
      "*   **First transaction (buy):** -5 books.\n",
      "*   **Restock transaction (add):** The number bought was 5, so double that is +10 books.\n",
      "*   **Second transaction (buy):** -8 books.\n",
      "*   **Total change:** -5 + 10 - 8 = 5 - 8 = -3 books.\n",
      "*   **Final state:** Initial + Total Change = 20 + (-3) = 17 books.\n",
      "\n",
      "**Expert 3: The Event Sequencer**\n",
      "\n",
      "1.  **Initial Count:** 20\n",
      "2.  **First Purchase:** 20 - 5 = 15\n",
      "3.  **Restock Calculation:** Number bought was 5. Double that is 10.\n",
      "4.  **Restock Applied:** 15 + 10 = 25\n",
      "5.  **Second Purchase:** 25 - 8 = 17\n",
      "\n",
      "---\n",
      "\n",
      "All three experts, reasoning through the problem using slightly different mental models (chronological, net change, event sequencing), arrive at the same conclusion because the problem is direct and sequential.\n",
      "\n",
      "**The final answer by majority vote is:**\n",
      "\n",
      "**17 books** are left on the shelf.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a3eb5f",
   "metadata": {},
   "source": [
    "**Role Playing prompting**\n",
    "\n",
    "Scenario: AI - powered Interview Coach\n",
    "Use Case: Job seekers use an AI to practice interview questions and receive feedback. Helps candidates prepare effectively for real interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5dd4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Act as an experienced hiring manager. Ask me five Interview questions one by one. After I respond, provide constructive feedback on my answer, including strengths and areas for improvement. If my response is incomplete, guide me toward a better answer.\"\n",
    "\n",
    "user_prompt = \"I'm preparing for a Data Scientist interview. Can you ask me fine five questions and evaluate my responses?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113013dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excellent! I'm ready to help you prepare. As an experienced hiring manager, I'll be looking for not just the technical correctness of your answers, but also your ability to communicate, problem-solve, and demonstrate business acumen.\n",
      "\n",
      "Let's start with your first question. Take your time to think about it.\n",
      "\n",
      "---\n",
      "\n",
      "**Question 1:** \"Tell me about a data science project you're particularly proud of. What was the problem you were trying to solve, what was your role, and what was the impact of your work?\"\n",
      "User is Speaking \n",
      "\n",
      "Gemini is Speaking: \n",
      "Thank you for sharing that project. It's good that you've identified a specific dataset and a general area of focus.\n",
      "\n",
      "Here's some constructive feedback on your response, along with guidance on how to strengthen it for future interviews:\n",
      "\n",
      "**Strengths:**\n",
      "*   You identified a specific dataset (Chicago crime) and a general area of interest (highest crime, frequency).\n",
      "*   You mentioned visualization, which is a key part of communicating data science insights.\n",
      "\n",
      "**Areas for Improvement:**\n",
      "\n",
      "1.  **Problem Definition:** \"Checking for the highest crime and frequency of occurrence\" sounds more like an initial exploratory data analysis (EDA) task rather than a complex data science *problem* with a clear objective.\n",
      "    *   **To improve:** Think about the *why* behind this analysis. What real-world question or business problem were you trying to answer? For example:\n",
      "        *   \"We aimed to identify crime hotspots to help allocate police resources more effectively.\"\n",
      "        *   \"We wanted to understand the temporal patterns of specific crime types to inform public safety campaigns.\"\n",
      "        *   \"We sought to predict future crime trends in certain areas to proactively deploy preventative measures.\"\n",
      "    *   **Guidance:** Frame the problem as a challenge that required data science to solve, not just a descriptive task.\n",
      "\n",
      "2.  **Your Specific Role & Methodology:** \"Work on the insights so that the results can be visualized\" is quite vague. It doesn't tell me *what* you actually did.\n",
      "    *   **To improve:** Be much more specific about your actions and the data science techniques you employed. Did you:\n",
      "        *   Clean and preprocess the data (e.g., handle missing values, outliers)?\n",
      "        *   Perform feature engineering (e.g., creating time-based features, spatial features)?\n",
      "        *   Use specific statistical methods or machine learning models (e.g., clustering for hotspots, time series analysis for trends, classification for predicting crime types)?\n",
      "        *   Design specific types of visualizations (e.g., heatmaps, interactive dashboards, time series plots)?\n",
      "        *   Communicate findings to stakeholders?\n",
      "    *   **Guidance:** Describe the *actions* you took and the *tools/methods* you used.\n",
      "\n",
      "3.  **Impact of Your Work:** \"The impact of my work made the goal achievable\" is a circular statement and doesn't convey any tangible value or outcome.\n",
      "    *   **To improve:** Quantify the impact whenever possible. What *specific* outcome resulted from your work? Did it lead to:\n",
      "        *   A change in policy or resource allocation?\n",
      "        *   A better understanding for stakeholders that led to a decision?\n",
      "        *   Improved efficiency or cost savings?\n",
      "        *   A new product feature or service?\n",
      "        *   For example: \"Our visualizations helped city officials identify specific neighborhoods experiencing a surge in property crime, leading to a targeted increase in police patrols in those areas, which correlated with a X% reduction in reported incidents over the next quarter.\"\n",
      "    *   **Guidance:** Focus on the *results* and the *value* your work delivered. How did it change something or help someone make a better decision?\n",
      "\n",
      "**To help you refine this answer, could you tell me a bit more about:**\n",
      "\n",
      "*   **What was the *specific question* you were trying to answer with the Chicago crime data?** (e.g., \"Which areas have the highest violent crime rates on weekends?\" or \"Can we predict the likelihood of a specific crime type occurring in a given district next month?\")\n",
      "*   **What specific data science techniques or tools did you use beyond basic counting?** (e.g., Python libraries like Pandas, Matplotlib, Seaborn, Scikit-learn; SQL; specific statistical tests; machine learning algorithms?)\n",
      "*   **What was the *tangible outcome* or *decision* that was influenced by your insights and visualizations?**\n",
      "\n",
      "Let's try to build a stronger narrative for this project.\n",
      "User is Speaking \n",
      "\n",
      "Gemini is Speaking: \n",
      "Excellent! This is a much clearer and more impactful problem statement. \"Helping the police department know the trends of crime that happen yearly and how to mitigate crime and increase security in the crime hotspots\" is a fantastic goal for a data science project. It clearly defines the *why* and the *who* (police department).\n",
      "\n",
      "Now, let's build on this strong foundation. To make your answer truly shine, we need to elaborate on the **\"how\"** and the **\"what happened next\"**.\n",
      "\n",
      "**Here's what we still need to flesh out:**\n",
      "\n",
      "1.  **Your Specific Role and Methodology (The \"How\"):**\n",
      "    *   You mentioned \"knowing the trends\" and \"identifying crime hotspots.\" How did you achieve this?\n",
      "    *   **What specific data science techniques did you use?** (e.g., time series analysis for yearly trends, spatial analysis/clustering for hotspots, statistical tests, machine learning models like K-means for clustering, regression for predicting future trends, etc.)\n",
      "    *   **What tools/libraries did you use?** (e.g., Python with Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, GeoPandas; R; SQL; Tableau for visualization?)\n",
      "    *   **What challenges did you face in the data (e.g., missing data, inconsistencies) and how did you overcome them?**\n",
      "    *   **How did you prepare the data for analysis?** (e.g., cleaning, feature engineering like creating time-based features or geographical features).\n",
      "\n",
      "2.  **The Impact of Your Work (The \"What Happened Next\"):**\n",
      "    *   You said the goal was to \"help the police department.\" How *exactly* did your work help them?\n",
      "    *   **What specific insights did you uncover?** (e.g., \"We found that violent crime significantly increased in district X during summer months,\" or \"Property crime was concentrated around commercial areas on weekdays.\")\n",
      "    *   **What actions did the police department take based on your findings?** (e.g., \"They reallocated patrols to specific areas,\" \"They launched a targeted community outreach program,\" \"They adjusted their shift schedules.\")\n",
      "    *   **Can you quantify the impact?** (e.g., \"This led to a X% reduction in reported incidents in those hotspots,\" or \"It improved resource allocation efficiency by Y%,\" or \"It informed a new public safety initiative.\") Even if it's not a direct number, describe the *qualitative* benefit.\n",
      "\n",
      "**Could you elaborate on these points? Specifically, tell me:**\n",
      "\n",
      "*   **What were the key data science techniques you applied to identify trends and hotspots?**\n",
      "*   **What was the most significant insight you uncovered?**\n",
      "*   **What was the tangible outcome or decision made by the police department as a direct result of your project?**\n",
      "User is Speaking \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUser is Speaking \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mYou: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     response = chat.send_message(user_input)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGemini is Speaking: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\working_with_LLMs\\llmenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1396\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1394\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_shell_context_var\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shell_parent_ident\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\working_with_LLMs\\llmenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1441\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1440\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1441\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config= genai.types.GenerateContentConfig(\n",
    "        system_instruction = system_prompt,\n",
    "        temperature=0.\n",
    "    )\n",
    ")\n",
    "response = chat.send_message(user_prompt)\n",
    "print(response.text)\n",
    "while True:\n",
    "    print(f\"User is Speaking \\n\")\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    response = chat.send_message(user_input)\n",
    "    print(f\"Gemini is Speaking: \\n{response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8002dd8e",
   "metadata": {},
   "source": [
    "Using python f\" string in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1117ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a birthday message for Timi, who is turning 14\n"
     ]
    }
   ],
   "source": [
    "name = \"Timi\"\n",
    "age = 14\n",
    "prompt = f\"Write a birthday message for {name}, who is turning {age}\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb221994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few options for a birthday message for Timi, turning 14, choose the one that best fits your relationship with them!\n",
      "\n",
      "**Option 1: Warm & Encouraging**\n",
      "\n",
      "Happy 14th Birthday, Timi!\n",
      "\n",
      "Wow, 14! What an amazing milestone. You're officially stepping further into your teenage years, a time of incredible growth, discovery, and excitement. This year, I hope you embrace every opportunity that comes your way. Keep exploring your passions, laughing often, and making amazing memories. Don't be afraid to dream big, try new things, and most importantly, always be wonderfully, uniquely YOU.\n",
      "\n",
      "Wishing you the happiest of birthdays filled with fun, gifts, and delicious cake! Here's to a fantastic year ahead!\n",
      "\n",
      "With lots of love,\n",
      "[Your Name]\n",
      "\n",
      "---\n",
      "\n",
      "**Option 2: Short & Sweet**\n",
      "\n",
      "Happy 14th Birthday, Timi!\n",
      "\n",
      "Can you believe it? 14! Wishing you a fantastic day filled with all your favorite things – fun, friends, gifts, and cake! May your year ahead be full of exciting new adventures and brilliant moments.\n",
      "\n",
      "Have an amazing day!\n",
      "[Your Name]\n",
      "\n",
      "---\n",
      "\n",
      "**Option 3: A Bit More Playful**\n",
      "\n",
      "Happy 14th Birthday, Timi!\n",
      "\n",
      "Woohoo, 14! You're officially leveling up! This is such a cool age, where you get to discover even more about yourself and the world around you. Hope your birthday is absolutely epic, packed with fun, good vibes, and everything you wished for.\n",
      "\n",
      "Here's to a super awesome year ahead!\n",
      "Best,\n",
      "[Your Name]\n",
      "\n",
      "---\n",
      "\n",
      "**Remember to add your name at the bottom!**\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)\n",
    "print(\"Birthday Message from AI:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe7210",
   "metadata": {},
   "source": [
    "Using Json Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fafc5398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt: \n",
      " My name is Zara. I love coding and want to learn AI. Write a friendly and short guide on how I can start\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "user_profile = {\n",
    "  \"name\": \"Zara\",\n",
    "  \"interest\": \"coding\",\n",
    "  \"goal\": \"learn AI\",\n",
    "  \"tone\": \"friendly\",\n",
    "  \"length\": \"short\"\n",
    "}\n",
    "\n",
    "prompt = f\"\"\"My name is {user_profile['name']}. I love {user_profile['interest']} and want to {user_profile['goal']}. Write a {user_profile['tone']} and {user_profile['length']} guide on how I can start\"\"\"\n",
    "\n",
    "print(\"Generated Prompt: \\n\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8a95d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Zara! That's awesome you love coding – you already have a fantastic starting point for AI! Think of AI as the next level of what your code can do.\n",
      "\n",
      "Here's a friendly, short guide to kickstart your AI journey:\n",
      "\n",
      "1.  **Strengthen Your Python Superpower:**\n",
      "    *   AI development heavily relies on Python. Since you love coding, really dive into Python fundamentals if you haven't already. Functions, loops, data structures – the works!\n",
      "\n",
      "2.  **Get Your Data Toolkit Ready:**\n",
      "    *   **NumPy:** For powerful numerical computing. Think of it as handling big lists of numbers super efficiently.\n",
      "    *   **Pandas:** For managing and analyzing data. It's like a spreadsheet in your code, making data easy to clean and explore.\n",
      "    *   *Why?* AI learns from data, and these tools help you prepare and understand it.\n",
      "\n",
      "3.  **Demystify Machine Learning Basics:**\n",
      "    *   **What is it?** AI often starts with Machine Learning (ML), where you teach computers to learn from examples without explicitly programming every single rule.\n",
      "    *   **Keywords to explore:** Supervised Learning (predicting based on labeled data) and Unsupervised Learning (finding patterns in unlabeled data). Don't worry about the math *too* much at first; focus on the concepts.\n",
      "    *   **Scikit-learn:** This Python library is your best friend for basic ML algorithms. You can build simple models with just a few lines of code!\n",
      "\n",
      "4.  **Start Small with Hands-On Projects:**\n",
      "    *   The best way to learn is by doing!\n",
      "    *   **Find simple datasets:** Websites like Kaggle (look for \"Titanic dataset\" or \"Iris dataset\" for beginner-friendly challenges) are full of them.\n",
      "    *   **Follow tutorials:** Pick a simple problem (like predicting if someone survived the Titanic or classifying types of flowers) and follow a beginner-level tutorial using NumPy, Pandas, and Scikit-learn.\n",
      "\n",
      "5.  **Explore Resources & Community:**\n",
      "    *   **Online Courses:** Look for free introductory courses on platforms like freeCodeCamp, Coursera (often has free audit options), or Codecademy.\n",
      "    *   **YouTube:** Channels like \"Krish Naik\" or \"Corey Schafer\" have great Python and ML tutorials.\n",
      "    *   **Community:** Join online forums or Discord servers for AI/ML beginners. Don't be afraid to ask questions!\n",
      "\n",
      "**Your Action Plan:**\n",
      "\n",
      "*   **Focus on Python:** Make sure you're solid.\n",
      "*   **Learn NumPy & Pandas:** Practice data manipulation.\n",
      "*   **Install Scikit-learn:** Play with simple ML models.\n",
      "*   **Pick a tiny project:** Follow a tutorial from start to finish.\n",
      "\n",
      "Remember, AI is a huge field, so be patient with yourself, celebrate small victories, and most importantly, **have fun exploring!** Happy coding, Zara!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)\n",
    "# print(\"Birthday Message from AI:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c8c1b",
   "metadata": {},
   "source": [
    "Application of Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03b1b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"@LS_King: Yo why'd you blow up my car?! @ThugLife99: Cry about it #GetRekt\"\"\"\n",
    "prompt = f\"\"\"Analyze this GTA online tweet fight:\n",
    "1. Who started it?\n",
    "2. Most toxic word?\n",
    "3. Suggested peace treaty (funny):\n",
    "```{text}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3f818fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down this classic GTA Online interaction:\n",
      "\n",
      "1.  **Who started it?**\n",
      "    @LS_King started the *tweet fight* by calling out @ThugLife99's in-game action. @ThugLife99, however, initiated the *in-game conflict* by blowing up the car.\n",
      "\n",
      "2.  **Most toxic word?**\n",
      "    The most toxic word is **\"Cry\"** (within \"Cry about it\"). It's dismissive, mocking, and aims to invalidate the other person's feelings, which is more personally hurtful than \"GetRekt,\" which is a boastful taunt.\n",
      "\n",
      "3.  **Suggested peace treaty (funny):**\n",
      "    Both players must enter Passive Mode and embark on a mandatory \"Friendly Drive-By.\" Instead of shooting, they have to drive past random NPCs and *compliment* their outfits/cars using only the in-game text chat, *without being sarcastic*. The first one to resort to violence or sarcasm has to buy the other's next car insurance claim *and* deliver them a fully modded Faggio.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)\n",
    "# print(\"Birthday Message from AI:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57c32d",
   "metadata": {},
   "source": [
    "FInd out how to build world cloud using python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c0257",
   "metadata": {},
   "source": [
    "Code Generation\n",
    "Scenario: AI Assisted code writing for developers\n",
    "use case: a juinor developer needs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be2eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71806e2e",
   "metadata": {},
   "source": [
    "Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ed515",
   "metadata": {},
   "source": [
    "Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1b8cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tesla is an electric vehicle company. Its stock price has increased by 20% in the last year. Apple is a technology company knowen for its iphones. Its stock price has increased by 15% in thelast year. Amazon is an ecommerce and cloud computing giant. Its stock price has increased by 10% in the last year\"\n",
    "prompt = f\"\"\"Convert the following information into a table format with columns for company, industry and stock performance:\n",
    "Text:```{text}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b38a67b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the information in a table format:\n",
      "\n",
      "| Company | Industry                  | Stock Performance             |\n",
      "|---------|---------------------------|-------------------------------|\n",
      "| Tesla   | Electric Vehicles         | Increased by 20% (last year)  |\n",
      "| Apple   | Technology                | Increased by 15% (last year)  |\n",
      "| Amazon  | E-commerce, Cloud Computing | Increased by 10% (last year)  |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26366508",
   "metadata": {},
   "source": [
    "Here's the information in a table format:\n",
    "\n",
    "| Company | Industry                  | Stock Performance             |\n",
    "|---------|---------------------------|-------------------------------|\n",
    "| Tesla   | Electric Vehicles         | Increased by 20% (last year)  |\n",
    "| Apple   | Technology                | Increased by 15% (last year)  |\n",
    "| Amazon  | E-commerce, Cloud Computing | Increased by 10% (last year)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3785465d",
   "metadata": {},
   "source": [
    "List Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40f24285",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Artificial Intelligence is transforming industries like healthcare, finance, and education by improving efficiency and decision making.\"\n",
    "prompt = f\"\"\"Format the response in a list:\n",
    "-Summarize the key industries AI is trandforming\n",
    "- Highlight its impact.\n",
    "\n",
    "Text: ```{text}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d5b0f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of the key industries AI is transforming and its impact:\n",
      "\n",
      "*   **Key Industries AI is Transforming:** Healthcare, finance, and education.\n",
      "*   **Impact:** Improving efficiency and decision-making within these industries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f49e7a",
   "metadata": {},
   "source": [
    "JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b78d825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"John is a data scientist with expertise in python and machine learning.\"\n",
    "prompt = f\"\"\"Extract key details from the text and return the output in JSON format.\n",
    "Text: ```{text}```\n",
    "output format:\n",
    "{{\n",
    "  \"name\":\",\n",
    "  \"profession\": \",\n",
    "  \"skills\":[]\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3c18b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"name\": \"John\",\n",
      "  \"profession\": \"data scientist\",\n",
      "  \"skills\": [\"python\", \"machine learning\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da19bb9",
   "metadata": {},
   "source": [
    "Strucutred Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8fda315",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a structured paragraph with clear headings and subheadings about the impact of a balanced diet on physical and mental health.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af286180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A balanced diet, rich in diverse nutrients, profoundly impacts both physical and mental health, serving as the cornerstone for overall well-being.\n",
      "\n",
      "**I. Physical Health Benefits**\n",
      "**Energy and Weight Management:** Proper intake of macronutrients (carbohydrates, proteins, fats) provides sustained energy, preventing crashes and supporting healthy weight management by regulating metabolism and satiety. **Disease Prevention and Immune Function:** Micronutrients (vitamins, minerals) and fiber are vital for bolstering the immune system and significantly reducing the risk of chronic conditions such as heart disease, type 2 diabetes, and certain cancers by supporting cellular repair and reducing inflammation.\n",
      "\n",
      "**II. Mental Well-being**\n",
      "**Mood Stability and Cognitive Performance:** Essential nutrients, including omega-3 fatty acids, B vitamins, and antioxidants, are crucial for neurotransmitter production, directly influencing mood regulation, reducing symptoms of anxiety and depression, and enhancing cognitive functions like memory, focus, and problem-solving. **Stress Resilience and Sleep Quality:** A nutrient-dense diet helps the body cope better with stress by supporting adrenal function and promoting better sleep quality, which are foundational for mental clarity and emotional resilience.\n",
      "\n",
      "Thus, prioritizing a balanced diet is an investment in a robust body and a resilient mind, fostering a higher quality of life.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c940635",
   "metadata": {},
   "source": [
    "Custom Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7fccffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In a distant kingdom, a brave knight named Arthur set out on a quest to find the elegendary sword of light. Through treacherous mountains and dark forests, he faced numerous challenges but remained determined to fulfill his destiny.\"\n",
    "\n",
    "prompt = f\"\"\"You will be provided with a text delimited by triple backticks.\n",
    "-If the text is short (around 20 words or fewer), genearte a suitable **Title**.\n",
    "- If the text is longer than 20 words, generate a concise **Summary**.\n",
    "\n",
    "Use the following format for the output:\n",
    "- Text: <provided text>\n",
    "- Title/Summary: <generated content>\n",
    "\n",
    "'''{text}'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724f2e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Text: In a distant kingdom, a brave knight named Arthur set out on a quest to find the elegendary sword of light. Through treacherous mountains and dark forests, he faced numerous challenges but remained determined to fulfill his destiny.\n",
      "- Title/Summary: Brave knight Arthur embarks on a perilous quest to find the legendary Sword of Light, facing many challenges with unwavering determination.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87182fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, let's begin. As an experienced hiring manager, I'm keen to understand your capabilities and how you might fit into our team.\n",
      "\n",
      "Let's start with a foundational question:\n",
      "\n",
      "**Question 1:** \"Tell me about yourself. Walk me through your professional journey and what brought you to this point in your career.\"Excellent, it sounds like you have some solid practical experience. That's a great foundation.\n",
      "\n",
      "Let's start with our first question.\n",
      "\n",
      "**Question 1:** Tell me about a challenging AI project you worked on during your fellowship. What was the core problem you were trying to solve, what approach did you take, and what was the outcome?Great to meet you! It sounds like you have some really interesting hands-on experience. That project you described, incorporating map data and location-based information to provide descriptions, news, and warnings about destinations, sounds particularly complex and impactful.\n",
      "\n",
      "Let's dive into your experience.\n",
      "\n",
      "**Question 1:** Could you elaborate on the most significant technical challenge you faced while building that AI model for describing destinations and incorporating map/location data? How did you approach solving it, and what was the outcome?"
     ]
    }
   ],
   "source": [
    "memory=[]\n",
    "def generate(prompt):\n",
    "    client = genai.Client(\n",
    "        api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    )\n",
    "    model = \"gemini-2.5-flash\"\n",
    "    new_prompt = types.Part.from_text(text=f\"\"\"{prompt}\"\"\")\n",
    "    memory.append(new_prompt)\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=memory,\n",
    "        ),\n",
    "    ]\n",
    "    tools = [\n",
    "        types.Tool(googleSearch=types.GoogleSearch(\n",
    "        )),\n",
    "    ]\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        # thinkingConfig: {\n",
    "        #     thinkingBudget: -1,\n",
    "        # },\n",
    "        tools=tools,\n",
    "        system_instruction=[\n",
    "            types.Part.from_text(text=\"\"\"Act as an experienced hiring manager. Ask me five interview questions one by one.\" \\\n",
    "\"After I respond, provide constructive feedback on my anwer, including strengths and areas for improvement. \" \\\n",
    "\"if my response is incomplete, guide me toward a better answer\"\"\"),\n",
    "        ],\n",
    "    )\n",
    "    for chunk in client.models.generate_content_stream(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    ):\n",
    "        print(chunk.text, end=\"\")\n",
    "while True:\n",
    "    userInput = input(\"\\nuser: \")\n",
    "    generate(userInput)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
