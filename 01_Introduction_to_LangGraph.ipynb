{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 1: Introduction to LangGraph & State Management\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what LangGraph is and why we need it\n",
    "- Learn core concepts: nodes, edges, state, checkpointers\n",
    "- Build a simple stateful chatbot with conversation memory\n",
    "- Compare LangGraph with LangChain chains\n",
    "\n",
    "**Prerequisites:** RAG with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: What is LangGraph?\n",
    "\n",
    "### The Problem with Chains\n",
    "\n",
    "You have learned about **LangChain chains** - they're great for fixed pipelines:\n",
    "\n",
    "```python\n",
    "# Always follows this path:\n",
    "chain = retriever | prompt | llm | parser\n",
    "```\n",
    "\n",
    "But what if you need:\n",
    "- **Decisions during execution?** (\"Should I retrieve or not?\")\n",
    "- **Loops and cycles?** (\"Try again if answer is poor\")\n",
    "- **Multiple tools?** (\"Use search OR calculator OR retrieval\")\n",
    "- **Complex control flow?** (\"If X then Y, else Z\")\n",
    "\n",
    "**That's where LangGraph comes in!**\n",
    "\n",
    "### LangGraph = State Machines for Agents\n",
    "\n",
    "LangGraph lets you build **graphs** where:\n",
    "- **Nodes** are functions that process state\n",
    "- **Edges** connect nodes (fixed or conditional)\n",
    "- **State** flows through the graph\n",
    "- **Agents make decisions** about which path to take\n",
    "\n",
    "```\n",
    "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  START â”€â”€â–¶â”‚  Node 1  â”‚\n",
    "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "           Decision?\n",
    "           â”œâ”€ YES â”€â–¶ Node 2 â”€â”€â”\n",
    "           â””â”€ NO  â”€â–¶ Node 3 â”€â”€â”¤\n",
    "                               â–¼\n",
    "                              END\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LangGraph Application Architecture](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/application.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install -q langgraph langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up OpenAI API Key\n",
    "\n",
    "Create a `.env` file in your project directory with:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found! Please set it in your .env file.\")\n",
    "\n",
    "print(\"âœ… API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the LLM\n",
    "\n",
    "We'll use **GPT-4o-mini** - it's fast and cost-effective for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"âœ… LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Core Concept #1 - State\n",
    "\n",
    "**State** â€œThe single source of truth for the whole agent execution.â€  \n",
    "**State** is the data that flows through your graph.\n",
    "\n",
    "### MessagesState\n",
    "\n",
    "For chatbots, LangGraph provides `MessagesState` - it stores conversation history:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Hello\"),\n",
    "        AIMessage(content=\"Hi! How can I help?\"),\n",
    "        HumanMessage(content=\"What's Python?\"),\n",
    "        # ... more messages\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "This is similar to `ConversationBufferMemory`, but more flexible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ¤” Reflection Question:** \n",
    "How is this different from ConversationBufferMemory? In chains, memory was managed separately. In LangGraph, it's part of the state that flows through nodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Core Concept #2 - Nodes\n",
    "\n",
    "**Nodes** are functions that process state and return updates.\n",
    "\n",
    "### Node Function Signature\n",
    "\n",
    "```python\n",
    "def my_node(state: MessagesState) -> dict:\n",
    "    # Process state\n",
    "    # Return updates to state\n",
    "    return {\"messages\": [new_message]}\n",
    "```\n",
    "\n",
    "### The Assistant Node\n",
    "\n",
    "Let's create our first node - it sends messages to the LLM and gets a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Assistant node defined\n"
     ]
    }
   ],
   "source": [
    "# System prompt that defines assistant behavior\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a friendly assistant that answers user questions. Be helpful and concise.\"\n",
    ")\n",
    "\n",
    "def assistant(state: MessagesState) -> dict:\n",
    "    \"\"\"\n",
    "    The assistant node - processes messages and generates response.\n",
    "    \"\"\"\n",
    "    # Combine system prompt with conversation history\n",
    "    messages = [sys_msg] + state[\"messages\"]\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Return as state update\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "print(\"âœ… Assistant node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ’¡ Key Point:** The node doesn't modify state directly - it returns updates that LangGraph applies automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a Retriever Node\n",
    "You don't need to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_docs(state: MessagesState):\n",
    "#     query = state[\"messages\"][-1].content  # latest HumanMessage\n",
    "#     docs = retriever.invoke(query)\n",
    "\n",
    "#     return {\n",
    "#         \"messages\": [\n",
    "#             ToolMessage(\n",
    "#                 content=\"\\n\".join(d.page_content for d in docs),\n",
    "#                 name=\"retriever\"\n",
    "#             )\n",
    "#         ]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Core Concept 2B - Edges\n",
    "\n",
    "**Edges** are the connections between nodes that control the flow of your agent.\n",
    "\n",
    "Think of edges as **roads** between cities (nodes). They determine which node to visit next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of Edges\n",
    "\n",
    "LangGraph has two types of edges:\n",
    "\n",
    "1. **Fixed/Static Edges** (Normal Edges)\n",
    "   - Always go from Node A to Node B\n",
    "   - No decision-making\n",
    "   - Used for predictable flows\n",
    "\n",
    "2. **Conditional Edges**\n",
    "   - Decide which node to visit next based on state\n",
    "   - Enable agent decision-making\n",
    "   - Used for dynamic, intelligent behavior\n",
    "\n",
    "```\n",
    "Fixed/Static Edge:\n",
    "  Node A â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Node B  (always goes to B)\n",
    "\n",
    "Conditional Edge:\n",
    "  Node A â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Decision?\n",
    "                    â”œâ”€ Condition 1 â”€â”€â–¶ Node B\n",
    "                    â”œâ”€ Condition 2 â”€â”€â–¶ Node C\n",
    "                    â””â”€ Condition 3 â”€â”€â–¶ Node D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Core Concept #3 - Building the Graph\n",
    "\n",
    "Now let's connect everything into a **StateGraph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Graph structure defined\n"
     ]
    }
   ],
   "source": [
    "# Create a StateGraph with MessagesState\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add the assistant node\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "\n",
    "# Define the flow:\n",
    "# START â†’ assistant â†’ END\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_edge(\"assistant\", END)\n",
    "\n",
    "print(\"âœ… Graph structure defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Flow\n",
    "\n",
    "```\n",
    "START â†’ [assistant node] â†’ END\n",
    "```\n",
    "\n",
    "- **START:** Entry point (receives user message)\n",
    "- **assistant:** Processes message and generates response\n",
    "- **END:** Exit point (returns final state)\n",
    "\n",
    "This is simple now, but later we'll add conditional edges for agentic behavior!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Core Concept #4 - Checkpointers (Memory)\n",
    "\n",
    "**Checkpointers** save state between interactions - this gives our agent memory!\n",
    "\n",
    "Without checkpointer:\n",
    "- Each call starts fresh\n",
    "- No conversation history\n",
    "- Agent forgets everything\n",
    "\n",
    "With checkpointer:\n",
    "- State persists between calls\n",
    "- Agent remembers conversation\n",
    "- Multi-turn conversations work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agent compiled with memory\n"
     ]
    }
   ],
   "source": [
    "# Create a memory checkpointer (stores in memory)\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph WITH memory\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"âœ… Agent compiled with memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ”§ Production Note:** `MemorySaver` stores in RAM (lost on restart). For production, use:\n",
    "- `SqliteSaver` - persists to SQLite database\n",
    "- `MongoDBSaver` - persists to MongoDB\n",
    "\n",
    "We'll use MemorySaver for learning since it's simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Visualizing the Graph\n",
    "\n",
    "One of LangGraph's best features - **visual representation** of your agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not display graph: Failed to reach https://mermaid.ink API while trying to render your graph after 1 retries. To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "Graph structure: START â†’ assistant â†’ END\n"
     ]
    }
   ],
   "source": [
    "# Visualize the graph structure\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"Graph structure: START â†’ assistant â†’ END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ’¡ Pro Tip:** Always visualize your graph! It helps you understand and debug agent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Running the Agent\n",
    "\n",
    "Now let's actually use our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session IDs (Thread IDs)\n",
    "\n",
    "Each conversation has a unique **thread_id**. Messages with the same thread_id share memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with session ID: chat-session-0012\n"
     ]
    }
   ],
   "source": [
    "# Define a session ID for this conversation\n",
    "session_id = \"chat-session-0012\"\n",
    "\n",
    "print(f\"Starting conversation with session ID: {session_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function for Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conversation function ready\n"
     ]
    }
   ],
   "source": [
    "def run_conversation(user_input: str, thread_id: str = session_id):\n",
    "    \"\"\"\n",
    "    Send a message to the agent and get response.\n",
    "    âš ï¸ WARNING: Using default thread_id shares conversation acrosss all calls!\n",
    "    In production, ALWAYS provide unique thread_id per user.\n",
    "    \"\"\"\n",
    "    # Invoke the agent\n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    \n",
    "    # Print the conversation\n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"\\nğŸ‘¤ User: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            print(f\"ğŸ¤– Agent: {message.content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"âœ… Conversation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Single Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ User: Hello! What's your name?\n",
      "ğŸ¤– Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can just call me Assistant. How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "run_conversation(\"Hello! What's your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Multi-Turn Conversation (Memory Test!)\n",
    "\n",
    "Now let's test if the agent remembers context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ User: Hello! What's your name?\n",
      "ğŸ¤– Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can just call me Assistant. How can I help you today?\n",
      "\n",
      "ğŸ‘¤ User: My favorite color is blue\n",
      "ğŸ¤– Agent: That's great! Blue is a calming and popular color. Do you have a specific shade of blue that you like, or do you enjoy it in general?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# First message\n",
    "run_conversation(\"My favorite color is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ User: Hello! What's your name?\n",
      "ğŸ¤– Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can just call me Assistant. How can I help you today?\n",
      "\n",
      "ğŸ‘¤ User: My favorite color is blue\n",
      "ğŸ¤– Agent: That's great! Blue is a calming and popular color. Do you have a specific shade of blue that you like, or do you enjoy it in general?\n",
      "\n",
      "ğŸ‘¤ User: What's my favorite color?\n",
      "ğŸ¤– Agent: Your favorite color is blue!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question - does it remember?\n",
    "run_conversation(\"What's my favorite color?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ User: What's my favorite color?\n",
      "ğŸ¤– Agent: I don't know your favorite color. If you tell me, I can remember it for our conversation!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "run_conversation(\"What's my favorite color?\", thread_id=\"111\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ‰ Success!** The agent remembered your favorite color because:\n",
    "1. The checkpointer saved the state after the first message\n",
    "2. The same thread_id retrieved that saved state\n",
    "3. The conversation history was passed to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Context-Dependent Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ User: Hello! What's your name?\n",
      "ğŸ¤– Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can just call me Assistant. How can I help you today?\n",
      "\n",
      "ğŸ‘¤ User: My favorite color is blue\n",
      "ğŸ¤– Agent: That's great! Blue is a calming and popular color. Do you have a specific shade of blue that you like, or do you enjoy it in general?\n",
      "\n",
      "ğŸ‘¤ User: What's my favorite color?\n",
      "ğŸ¤– Agent: Your favorite color is blue!\n",
      "\n",
      "ğŸ‘¤ User: I'm learning about RAG systems\n",
      "ğŸ¤– Agent: That's interesting! RAG systems, or Retrieval-Augmented Generation systems, combine information retrieval with generative models. They typically retrieve relevant information from a database or knowledge base to enhance the responses generated by a language model. This can improve accuracy and provide more contextually relevant answers. Do you have specific questions about RAG systems or how they work?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Start a new topic\n",
    "run_conversation(\"I'm learning about RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ User: Hello! What's your name?\n",
      "ğŸ¤– Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can just call me Assistant. How can I help you today?\n",
      "\n",
      "ğŸ‘¤ User: My favorite color is blue\n",
      "ğŸ¤– Agent: That's great! Blue is a calming and popular color. Do you have a specific shade of blue that you like, or do you enjoy it in general?\n",
      "\n",
      "ğŸ‘¤ User: What's my favorite color?\n",
      "ğŸ¤– Agent: Your favorite color is blue!\n",
      "\n",
      "ğŸ‘¤ User: I'm learning about RAG systems\n",
      "ğŸ¤– Agent: That's interesting! RAG systems, or Retrieval-Augmented Generation systems, combine information retrieval with generative models. They typically retrieve relevant information from a database or knowledge base to enhance the responses generated by a language model. This can improve accuracy and provide more contextually relevant answers. Do you have specific questions about RAG systems or how they work?\n",
      "\n",
      "ğŸ‘¤ User: Can you explain the main components?\n",
      "ğŸ¤– Agent: Sure! The main components of a Retrieval-Augmented Generation (RAG) system typically include:\n",
      "\n",
      "1. **Retriever**: This component searches a large corpus of documents or a knowledge base to find relevant information based on a given query. It can use techniques like keyword matching or embeddings to retrieve the most pertinent documents.\n",
      "\n",
      "2. **Generator**: Once the retriever has identified relevant documents, the generator takes these documents and the original query to produce a coherent and contextually appropriate response. This is usually done using a language model.\n",
      "\n",
      "3. **Knowledge Base**: This is the collection of documents or data that the retriever accesses to find relevant information. It can include structured data, unstructured text, or a combination of both.\n",
      "\n",
      "4. **Pipeline**: The overall architecture that connects the retriever and generator, ensuring that the output of the retriever is fed into the generator effectively.\n",
      "\n",
      "5. **Training Mechanism**: RAG systems often require special training methods that involve both retrieval and generation tasks to optimize their performance.\n",
      "\n",
      "These components work together to enhance the quality of generated responses by grounding them in factual information. If you have more specific questions about any of these components, feel free to ask!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reference it\n",
    "run_conversation(\"Can you explain the main components?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ User: Hello! What's your name?\n",
      "ğŸ¤– Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can just call me Assistant. How can I help you today?\n",
      "\n",
      "ğŸ‘¤ User: My favorite color is blue\n",
      "ğŸ¤– Agent: That's great! Blue is a calming and popular color. Do you have a specific shade of blue that you like, or do you enjoy it in general?\n",
      "\n",
      "ğŸ‘¤ User: What's my favorite color?\n",
      "ğŸ¤– Agent: Your favorite color is blue!\n",
      "\n",
      "ğŸ‘¤ User: I'm learning about RAG systems\n",
      "ğŸ¤– Agent: That's interesting! RAG systems, or Retrieval-Augmented Generation systems, combine information retrieval with generative models. They typically retrieve relevant information from a database or knowledge base to enhance the responses generated by a language model. This can improve accuracy and provide more contextually relevant answers. Do you have specific questions about RAG systems or how they work?\n",
      "\n",
      "ğŸ‘¤ User: Can you explain the main components?\n",
      "ğŸ¤– Agent: Sure! The main components of a Retrieval-Augmented Generation (RAG) system typically include:\n",
      "\n",
      "1. **Retriever**: This component searches a large corpus of documents or a knowledge base to find relevant information based on a given query. It can use techniques like keyword matching or embeddings to retrieve the most pertinent documents.\n",
      "\n",
      "2. **Generator**: Once the retriever has identified relevant documents, the generator takes these documents and the original query to produce a coherent and contextually appropriate response. This is usually done using a language model.\n",
      "\n",
      "3. **Knowledge Base**: This is the collection of documents or data that the retriever accesses to find relevant information. It can include structured data, unstructured text, or a combination of both.\n",
      "\n",
      "4. **Pipeline**: The overall architecture that connects the retriever and generator, ensuring that the output of the retriever is fed into the generator effectively.\n",
      "\n",
      "5. **Training Mechanism**: RAG systems often require special training methods that involve both retrieval and generation tasks to optimize their performance.\n",
      "\n",
      "These components work together to enhance the quality of generated responses by grounding them in factual information. If you have more specific questions about any of these components, feel free to ask!\n",
      "\n",
      "ğŸ‘¤ User: Which component is most important?\n",
      "ğŸ¤– Agent: The importance of each component in a RAG system can depend on the specific use case, but generally:\n",
      "\n",
      "- **Retriever**: This is crucial because the quality of the retrieved information directly impacts the relevance and accuracy of the generated response. If the retriever pulls in irrelevant or low-quality documents, the generator will have a harder time producing a useful answer.\n",
      "\n",
      "- **Generator**: This component is also vital, as it synthesizes the retrieved information into a coherent response. A strong generative model can enhance the overall quality of the output, even if the retrieved documents are somewhat relevant.\n",
      "\n",
      "In summary, both components are interdependent, and the effectiveness of a RAG system often hinges on the balance between a good retriever and a good generator. In practice, optimizing both will yield the best results.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "run_conversation(\"Which component is most important?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ’¡ Notice:** The agent maintains context across multiple turns - just like a real conversation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Multiple Conversations (Different Thread IDs)\n",
    "\n",
    "Let's test that different thread IDs have separate memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”µ CONVERSATION 1\n",
      "\n",
      "ğŸ‘¤ User: My name is Alice\n",
      "ğŸ¤– Agent: Nice to meet you, Alice! How can I assist you today?\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ğŸŸ¢ CONVERSATION 2\n",
      "\n",
      "ğŸ‘¤ User: My name is Bob\n",
      "ğŸ¤– Agent: Nice to meet you, Bob! How can I assist you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Conversation 1\n",
    "print(\"\\nğŸ”µ CONVERSATION 1\")\n",
    "run_conversation(\"My name is Alice\", thread_id=\"user_alicee\")\n",
    "\n",
    "# Conversation 2 (different user)\n",
    "print(\"\\nğŸŸ¢ CONVERSATION 2\")\n",
    "run_conversation(\"My name is Bob\", thread_id=\"user_bobb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”µ BACK TO CONVERSATION 1\n",
      "\n",
      "ğŸ‘¤ User: My name is Alice\n",
      "ğŸ¤– Agent: Nice to meet you, Alice! How can I assist you today?\n",
      "\n",
      "ğŸ‘¤ User: What's my name?\n",
      "ğŸ¤– Agent: Your name is Alice! How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Back to Alice - does it remember her name?\n",
    "print(\"\\nğŸ”µ BACK TO CONVERSATION 1\")\n",
    "run_conversation(\"What's my name?\", thread_id=\"user_alicee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ BACK TO CONVERSATION 2\n",
      "\n",
      "ğŸ‘¤ User: My name is Bob\n",
      "ğŸ¤– Agent: Nice to meet you, Bob! How can I assist you today?\n",
      "\n",
      "ğŸ‘¤ User: What's my name?\n",
      "ğŸ¤– Agent: Your name is Bob!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Back to Bob\n",
    "print(\"\\nğŸŸ¢ BACK TO CONVERSATION 2\")\n",
    "run_conversation(\"What's my name?\", thread_id=\"user_bobb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ¯ Key Insight:** Each thread_id maintains its own conversation history. This is how you'd handle multiple users in production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Interactive Chat Loop\n",
    "\n",
    "Let's create an interactive chat session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ¤– Interactive Chat Started\n",
      "Type your message and press Enter. Type 'exit' to quit.\n",
      "======================================================================\n",
      "\n",
      "\n",
      "ğŸ‘¤ User: what isan agent?\n",
      "ğŸ¤– Agent: An agent is a person or entity that acts on behalf of another, known as the principal, to perform specific tasks or make decisions. Agents are commonly used in various fields, including business, real estate, sports, and entertainment. They can represent clients in negotiations, manage contracts, or provide expertise in specific areas. In a broader sense, \"agent\" can also refer to a software program that acts autonomously to perform tasks on behalf of a user.\n",
      "\n",
      "ğŸ‘¤ User: What is an LLM agent?\n",
      "ğŸ¤– Agent: An LLM agent refers to a system or application that utilizes a large language model (LLM) to perform tasks or assist users in various ways. LLMs, like OpenAI's GPT models, are designed to understand and generate human-like text based on the input they receive.\n",
      "\n",
      "LLM agents can be used for purposes such as:\n",
      "\n",
      "1. **Conversational Agents**: Engaging in dialogue with users, answering questions, and providing information.\n",
      "2. **Content Generation**: Creating articles, summaries, or other written materials based on prompts.\n",
      "3. **Task Automation**: Performing specific tasks like scheduling, data analysis, or coding assistance based on natural language commands.\n",
      "4. **Personal Assistants**: Helping users manage their daily activities, reminders, and other personal tasks.\n",
      "\n",
      "These agents leverage the capabilities of LLMs to understand context, generate relevant responses, and improve user interaction.\n",
      "\n",
      "ğŸ‘¤ User: what isan agent?\n",
      "ğŸ¤– Agent: An agent is a person or entity that acts on behalf of another, known as the principal, to perform specific tasks or make decisions. Agents are commonly used in various fields, including business, real estate, sports, and entertainment. They can represent clients in negotiations, manage contracts, or provide expertise in specific areas. In a broader sense, \"agent\" can also refer to a software program that acts autonomously to perform tasks on behalf of a user.\n",
      "\n",
      "ğŸ‘¤ User: What is an LLM agent?\n",
      "ğŸ¤– Agent: An LLM agent refers to a system or application that utilizes a large language model (LLM) to perform tasks or assist users in various ways. LLMs, like OpenAI's GPT models, are designed to understand and generate human-like text based on the input they receive.\n",
      "\n",
      "LLM agents can be used for purposes such as:\n",
      "\n",
      "1. **Conversational Agents**: Engaging in dialogue with users, answering questions, and providing information.\n",
      "2. **Content Generation**: Creating articles, summaries, or other written materials based on prompts.\n",
      "3. **Task Automation**: Performing specific tasks like scheduling, data analysis, or coding assistance based on natural language commands.\n",
      "4. **Personal Assistants**: Helping users manage their daily activities, reminders, and other personal tasks.\n",
      "\n",
      "These agents leverage the capabilities of LLMs to understand context, generate relevant responses, and improve user interaction.\n",
      "\n",
      "ğŸ‘¤ User: Okay, thta answer was good, Kudos to you on that\n",
      "ğŸ¤– Agent: Thank you! I'm glad you found the answer helpful. If you have any more questions or need further information, feel free to ask!\n",
      "\n",
      "ğŸ‘¤ User: what isan agent?\n",
      "ğŸ¤– Agent: An agent is a person or entity that acts on behalf of another, known as the principal, to perform specific tasks or make decisions. Agents are commonly used in various fields, including business, real estate, sports, and entertainment. They can represent clients in negotiations, manage contracts, or provide expertise in specific areas. In a broader sense, \"agent\" can also refer to a software program that acts autonomously to perform tasks on behalf of a user.\n",
      "\n",
      "ğŸ‘¤ User: What is an LLM agent?\n",
      "ğŸ¤– Agent: An LLM agent refers to a system or application that utilizes a large language model (LLM) to perform tasks or assist users in various ways. LLMs, like OpenAI's GPT models, are designed to understand and generate human-like text based on the input they receive.\n",
      "\n",
      "LLM agents can be used for purposes such as:\n",
      "\n",
      "1. **Conversational Agents**: Engaging in dialogue with users, answering questions, and providing information.\n",
      "2. **Content Generation**: Creating articles, summaries, or other written materials based on prompts.\n",
      "3. **Task Automation**: Performing specific tasks like scheduling, data analysis, or coding assistance based on natural language commands.\n",
      "4. **Personal Assistants**: Helping users manage their daily activities, reminders, and other personal tasks.\n",
      "\n",
      "These agents leverage the capabilities of LLMs to understand context, generate relevant responses, and improve user interaction.\n",
      "\n",
      "ğŸ‘¤ User: Okay, thta answer was good, Kudos to you on that\n",
      "ğŸ¤– Agent: Thank you! I'm glad you found the answer helpful. If you have any more questions or need further information, feel free to ask!\n",
      "\n",
      "ğŸ‘¤ User: I am just checking to see if you can hold a conversation. Which model is best for semantic reranking?\n",
      "ğŸ¤– Agent: For semantic reranking, models that excel at understanding context and relationships between texts are commonly used. Some of the best options include:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Particularly effective for understanding context in sentences, making it suitable for reranking tasks.\n",
      "\n",
      "2. **RoBERTa**: An optimized version of BERT that often performs better due to more extensive training on diverse data.\n",
      "\n",
      "3. **T5 (Text-to-Text Transfer Transformer)**: This model treats all NLP tasks as text-to-text, allowing it to be fine-tuned for semantic reranking effectively.\n",
      "\n",
      "4. **ColBERT (Contextualized Late Interaction over BERT)**: Specifically designed for efficient semantic search and reranking.\n",
      "\n",
      "5. **Sentence-BERT (SBERT)**: A modification of BERT that produces semantically meaningful sentence embeddings, making it useful for reranking similar sentences or documents.\n",
      "\n",
      "The choice of the best model can depend on the specific dataset, task requirements, and computational resources available. Fine-tuning these models on your particular data can also significantly enhance their performance.\n",
      "\n",
      "ğŸ‘¤ User: what isan agent?\n",
      "ğŸ¤– Agent: An agent is a person or entity that acts on behalf of another, known as the principal, to perform specific tasks or make decisions. Agents are commonly used in various fields, including business, real estate, sports, and entertainment. They can represent clients in negotiations, manage contracts, or provide expertise in specific areas. In a broader sense, \"agent\" can also refer to a software program that acts autonomously to perform tasks on behalf of a user.\n",
      "\n",
      "ğŸ‘¤ User: What is an LLM agent?\n",
      "ğŸ¤– Agent: An LLM agent refers to a system or application that utilizes a large language model (LLM) to perform tasks or assist users in various ways. LLMs, like OpenAI's GPT models, are designed to understand and generate human-like text based on the input they receive.\n",
      "\n",
      "LLM agents can be used for purposes such as:\n",
      "\n",
      "1. **Conversational Agents**: Engaging in dialogue with users, answering questions, and providing information.\n",
      "2. **Content Generation**: Creating articles, summaries, or other written materials based on prompts.\n",
      "3. **Task Automation**: Performing specific tasks like scheduling, data analysis, or coding assistance based on natural language commands.\n",
      "4. **Personal Assistants**: Helping users manage their daily activities, reminders, and other personal tasks.\n",
      "\n",
      "These agents leverage the capabilities of LLMs to understand context, generate relevant responses, and improve user interaction.\n",
      "\n",
      "ğŸ‘¤ User: Okay, thta answer was good, Kudos to you on that\n",
      "ğŸ¤– Agent: Thank you! I'm glad you found the answer helpful. If you have any more questions or need further information, feel free to ask!\n",
      "\n",
      "ğŸ‘¤ User: I am just checking to see if you can hold a conversation. Which model is best for semantic reranking?\n",
      "ğŸ¤– Agent: For semantic reranking, models that excel at understanding context and relationships between texts are commonly used. Some of the best options include:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Particularly effective for understanding context in sentences, making it suitable for reranking tasks.\n",
      "\n",
      "2. **RoBERTa**: An optimized version of BERT that often performs better due to more extensive training on diverse data.\n",
      "\n",
      "3. **T5 (Text-to-Text Transfer Transformer)**: This model treats all NLP tasks as text-to-text, allowing it to be fine-tuned for semantic reranking effectively.\n",
      "\n",
      "4. **ColBERT (Contextualized Late Interaction over BERT)**: Specifically designed for efficient semantic search and reranking.\n",
      "\n",
      "5. **Sentence-BERT (SBERT)**: A modification of BERT that produces semantically meaningful sentence embeddings, making it useful for reranking similar sentences or documents.\n",
      "\n",
      "The choice of the best model can depend on the specific dataset, task requirements, and computational resources available. Fine-tuning these models on your particular data can also significantly enhance their performance.\n",
      "\n",
      "ğŸ‘¤ User: what about cross encoder models\n",
      "ğŸ¤– Agent: Cross-encoder models are another powerful approach for tasks like semantic reranking. Unlike traditional methods that encode inputs separately (e.g., query and document), cross-encoders process both inputs together in a single pass through the model. This allows them to better capture the interaction between the query and the document, leading to more accurate relevance scoring.\n",
      "\n",
      "### Advantages of Cross-Encoder Models:\n",
      "\n",
      "1. **Contextual Understanding**: They can leverage the full context of both the query and the document, improving the quality of the embeddings generated.\n",
      "\n",
      "2. **Higher Accuracy**: Cross-encoders often achieve better performance on reranking tasks because they consider the entire input context simultaneously.\n",
      "\n",
      "3. **Fine-Grained Scoring**: They can provide more nuanced relevance scores based on the interaction between the query and the document.\n",
      "\n",
      "### Popular Cross-Encoder Models:\n",
      "\n",
      "1. **BERT**: Can be used as a cross-encoder by feeding both the query and the document into the same model.\n",
      "2. **RoBERTa**: Like BERT, it can be adapted for cross-encoding.\n",
      "3. **Cross-Encoder BERT**: Specifically designed for tasks requiring the interaction of two inputs.\n",
      "\n",
      "### Considerations:\n",
      "\n",
      "- **Computationally Intensive**: Cross-encoders require more computational resources since they process each query-document pair together, which can be slower than using separate encoders.\n",
      "- **Scalability**: They are generally less scalable for large retrieval tasks, where a large number of documents need to be ranked, unless combined with efficient retrieval methods.\n",
      "\n",
      "In practice, many systems use a combination of a bi-encoder (for initial filtering) followed by a cross-encoder (for final ranking) to balance efficiency and accuracy.\n",
      "\n",
      "ğŸ‘¤ User: what isan agent?\n",
      "ğŸ¤– Agent: An agent is a person or entity that acts on behalf of another, known as the principal, to perform specific tasks or make decisions. Agents are commonly used in various fields, including business, real estate, sports, and entertainment. They can represent clients in negotiations, manage contracts, or provide expertise in specific areas. In a broader sense, \"agent\" can also refer to a software program that acts autonomously to perform tasks on behalf of a user.\n",
      "\n",
      "ğŸ‘¤ User: What is an LLM agent?\n",
      "ğŸ¤– Agent: An LLM agent refers to a system or application that utilizes a large language model (LLM) to perform tasks or assist users in various ways. LLMs, like OpenAI's GPT models, are designed to understand and generate human-like text based on the input they receive.\n",
      "\n",
      "LLM agents can be used for purposes such as:\n",
      "\n",
      "1. **Conversational Agents**: Engaging in dialogue with users, answering questions, and providing information.\n",
      "2. **Content Generation**: Creating articles, summaries, or other written materials based on prompts.\n",
      "3. **Task Automation**: Performing specific tasks like scheduling, data analysis, or coding assistance based on natural language commands.\n",
      "4. **Personal Assistants**: Helping users manage their daily activities, reminders, and other personal tasks.\n",
      "\n",
      "These agents leverage the capabilities of LLMs to understand context, generate relevant responses, and improve user interaction.\n",
      "\n",
      "ğŸ‘¤ User: Okay, thta answer was good, Kudos to you on that\n",
      "ğŸ¤– Agent: Thank you! I'm glad you found the answer helpful. If you have any more questions or need further information, feel free to ask!\n",
      "\n",
      "ğŸ‘¤ User: I am just checking to see if you can hold a conversation. Which model is best for semantic reranking?\n",
      "ğŸ¤– Agent: For semantic reranking, models that excel at understanding context and relationships between texts are commonly used. Some of the best options include:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Particularly effective for understanding context in sentences, making it suitable for reranking tasks.\n",
      "\n",
      "2. **RoBERTa**: An optimized version of BERT that often performs better due to more extensive training on diverse data.\n",
      "\n",
      "3. **T5 (Text-to-Text Transfer Transformer)**: This model treats all NLP tasks as text-to-text, allowing it to be fine-tuned for semantic reranking effectively.\n",
      "\n",
      "4. **ColBERT (Contextualized Late Interaction over BERT)**: Specifically designed for efficient semantic search and reranking.\n",
      "\n",
      "5. **Sentence-BERT (SBERT)**: A modification of BERT that produces semantically meaningful sentence embeddings, making it useful for reranking similar sentences or documents.\n",
      "\n",
      "The choice of the best model can depend on the specific dataset, task requirements, and computational resources available. Fine-tuning these models on your particular data can also significantly enhance their performance.\n",
      "\n",
      "ğŸ‘¤ User: what about cross encoder models\n",
      "ğŸ¤– Agent: Cross-encoder models are another powerful approach for tasks like semantic reranking. Unlike traditional methods that encode inputs separately (e.g., query and document), cross-encoders process both inputs together in a single pass through the model. This allows them to better capture the interaction between the query and the document, leading to more accurate relevance scoring.\n",
      "\n",
      "### Advantages of Cross-Encoder Models:\n",
      "\n",
      "1. **Contextual Understanding**: They can leverage the full context of both the query and the document, improving the quality of the embeddings generated.\n",
      "\n",
      "2. **Higher Accuracy**: Cross-encoders often achieve better performance on reranking tasks because they consider the entire input context simultaneously.\n",
      "\n",
      "3. **Fine-Grained Scoring**: They can provide more nuanced relevance scores based on the interaction between the query and the document.\n",
      "\n",
      "### Popular Cross-Encoder Models:\n",
      "\n",
      "1. **BERT**: Can be used as a cross-encoder by feeding both the query and the document into the same model.\n",
      "2. **RoBERTa**: Like BERT, it can be adapted for cross-encoding.\n",
      "3. **Cross-Encoder BERT**: Specifically designed for tasks requiring the interaction of two inputs.\n",
      "\n",
      "### Considerations:\n",
      "\n",
      "- **Computationally Intensive**: Cross-encoders require more computational resources since they process each query-document pair together, which can be slower than using separate encoders.\n",
      "- **Scalability**: They are generally less scalable for large retrieval tasks, where a large number of documents need to be ranked, unless combined with efficient retrieval methods.\n",
      "\n",
      "In practice, many systems use a combination of a bi-encoder (for initial filtering) followed by a cross-encoder (for final ranking) to balance efficiency and accuracy.\n",
      "\n",
      "ğŸ‘¤ User: what about cross encoder models\n",
      "ğŸ¤– Agent: Cross-encoder models are a type of neural network architecture designed to evaluate the relationship between two pieces of textâ€”typically a query and a documentâ€”by processing them together in a single forward pass. This approach allows the model to capture interactions and dependencies between the two inputs more effectively than separate encoding methods.\n",
      "\n",
      "### Key Features of Cross-Encoder Models:\n",
      "\n",
      "1. **Joint Processing**: Both the query and the document are concatenated and fed into the model together, allowing the architecture to understand how they relate to each other in context.\n",
      "\n",
      "2. **Higher Accuracy**: Cross-encoders generally provide better performance for tasks like semantic reranking because they consider the full context of both inputs simultaneously, leading to more nuanced relevance scoring.\n",
      "\n",
      "3. **Fine-Grained Relevance Scoring**: They can produce detailed relevance scores based on the interaction between the query and the document, which is particularly useful for determining the best matches in information retrieval tasks.\n",
      "\n",
      "### Popular Cross-Encoder Models:\n",
      "\n",
      "- **BERT**: The original BERT architecture can be adapted as a cross-encoder by inputting the query and document together.\n",
      "- **RoBERTa**: An optimized version of BERT, it can also be used in a cross-encoder configuration.\n",
      "- **DistilBERT**: A smaller, faster version of BERT that can be employed as a cross-encoder while still retaining a lot of its performance benefits.\n",
      "\n",
      "### Considerations:\n",
      "\n",
      "- **Computational Cost**: Cross-encoders tend to be more computationally intensive than bi-encoders (which process inputs separately). This can make them slower and less scalable for large datasets, especially when dealing with many documents.\n",
      "  \n",
      "- **Use Cases**: They are particularly effective in scenarios where high accuracy is required, such as reranking results after an initial retrieval step. A common approach is to use a bi-encoder for fast retrieval of candidate documents, followed by a cross-encoder for final ranking.\n",
      "\n",
      "In summary, cross-encoders are a powerful tool for tasks involving semantic understanding and ranking, particularly when the interactions between inputs are critical to performance.\n",
      "\n",
      "ğŸ‘‹ Goodbye!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Run an interactive chat session.\n",
    "    Type 'exit' or 'quit' to stop.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ¤– Interactive Chat Started\")\n",
    "    print(\"Type your message and press Enter. Type 'exit' to quit.\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    thread_id = \"interactive_session2\"\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nğŸ‘¤ You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nğŸ‘‹ Goodbye!\\n\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Get response\n",
    "        result = agent.invoke(\n",
    "            {\"messages\": [HumanMessage(content=user_input)]},\n",
    "            config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "        )\n",
    "\n",
    "        # Print the conversation\n",
    "        for message in result[\"messages\"]:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                print(f\"\\nğŸ‘¤ User: {message.content}\")\n",
    "            elif isinstance(message, AIMessage):\n",
    "                print(f\"ğŸ¤– Agent: {message.content}\")\n",
    "\n",
    "\n",
    "        # # Print agent's response\n",
    "        # agent_message = result[\"messages\"][-1]\n",
    "        # print(f\"\\nğŸ¤– Agent: {agent_message.content}\")\n",
    "\n",
    "# Uncomment to run interactive chat:\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: ConversationalRetrievalChain\n",
    "\n",
    "Let's compare LangGraph with the memory you learned in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationalRetrievalChain\n",
    "\n",
    "```python\n",
    "# Langchain approach\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is Python?\"})\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- âœ… Simple to use\n",
    "- âœ… Built-in memory\n",
    "- âŒ Fixed pipeline (always retrieves)\n",
    "- âŒ No conditional logic\n",
    "- âŒ Can't add complex decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 12: LangGraph Agent\n",
    "\n",
    "```python\n",
    "# LangGraph approach\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"assistant\", assistant_node)\n",
    "memory = MemorySaver()\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is Python?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"user_123\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- âœ… Flexible - add any nodes/edges\n",
    "- âœ… Conditional logic (coming in Topic 2)\n",
    "- âœ… Agents can make decisions\n",
    "- âœ… Supports cycles and loops\n",
    "- âŒ More complex to set up\n",
    "- âŒ Requires understanding graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What?\n",
    "\n",
    "| Use Case | LangChains | LangGraph |\n",
    "|----------|-------------------|------------------------|\n",
    "| Simple chatbot | âœ… Perfect | âš ï¸ Overkill |\n",
    "| Fixed RAG pipeline | âœ… Great | âš ï¸ Unnecessary |\n",
    "| Agent with tools | âŒ Limited | âœ… Ideal |\n",
    "| Conditional retrieval | âŒ Can't do | âœ… Perfect |\n",
    "| Multi-agent systems | âŒ Not possible | âœ… Built for it |\n",
    "\n",
    "**Rule of thumb:** If you need decision-making during execution, use LangGraph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **LangGraph Basics**\n",
    "   - LangGraph enables agentic behavior through graphs\n",
    "   - Better than chains when you need decisions during execution\n",
    "\n",
    "2. **Core Concepts**\n",
    "   - **State:** Data flowing through the graph (MessagesState for chat)\n",
    "   - **Nodes:** Functions that process and update state\n",
    "   - **Edges:** Connections between nodes (fixed or conditional)\n",
    "   - **Checkpointers:** Persist state for memory across sessions\n",
    "\n",
    "3. **Practical Skills**\n",
    "   - Built a stateful chatbot\n",
    "   - Used thread_id for multi-user conversations\n",
    "   - Visualized graph structure\n",
    "   - Compared with Module 9 chains\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Topic 2: Tool Integration**\n",
    "- Add tools for agents (search, calculator, retrieval)\n",
    "- Conditional edges (agent decides which tool to use)\n",
    "- This is where LangGraph really shines!\n",
    "\n",
    "**Topic 3: Agentic RAG**\n",
    "- Agent that decides when to retrieve\n",
    "- Combines everything from Topics 1-2\n",
    "- The core concept of this module!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Practice Exercises\n",
    "## Exercise 1: Build Your First Stateful Agent\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "**Estimated Time:** 30-45 minutes\n",
    "\n",
    "### Task\n",
    "Build a simple customer support chatbot that remembers conversation context.\n",
    "\n",
    "### Requirements\n",
    "1. Create a StateGraph with MessagesState\n",
    "2. Add a system prompt that makes the agent act as a helpful customer support rep\n",
    "3. Use MemorySaver checkpointer for memory\n",
    "4. Test with a multi-turn conversation where context matters\n",
    "\n",
    "### Example Conversation\n",
    "```\n",
    "User: \"I bought a laptop last week\"\n",
    "Agent: \"I'd be happy to help with your laptop! What seems to be the issue?\"\n",
    "User: \"It won't turn on\"\n",
    "Agent: \"I understand your laptop won't turn on. Have you tried...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection Questions\n",
    "\n",
    "1. **How does LangGraph's state management differ from ConversationalRetrievalChain memory?**\n",
    "   \n",
    "2. **Why do we need thread_id for conversations?**\n",
    "   \n",
    "3. **What happens if you don't configure a checkpointer?**\n",
    "   \n",
    "4. **When would you choose chains over LangGraph?**\n",
    "   \n",
    "5. **How would you debug an agent that's not behaving correctly?**\n",
    "\n",
    "Write your answers below or discuss with your study group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**ğŸ‰ Topic 1 Complete!** \n",
    "\n",
    "You now understand LangGraph fundamentals. Next up: **Tool Integration** - where agents become truly powerful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
